---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.name }}-hadoop-config
  labels:
    app: benchmarks
    chart: hadoop-0.0.1
    release: foo
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
data:
  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
          <name>fs.adl.impl</name>
          <value>org.apache.hadoop.fs.adl.AdlFileSystem</value>
      </property>
      <property>
          <name>fs.AbstractFileSystem.adl.impl</name>
          <value>org.apache.hadoop.fs.adl.Adl</value>
          <final>true</final>
      </property>
      <property>
          <name>fs.adl.impl</name>
          <value>org.apache.hadoop.fs.adl.AdlFileSystem</value>
      </property>
      <property>
          <name>fs.AbstractFileSystem.adl.impl</name>
          <value>org.apache.hadoop.fs.adl.Adl</value>
      </property>
      <property>
          <name>fs.adl.oauth2.access.token.provider.type</name>
          <value>ClientCredential</value>
      </property>
      <property>
          <name>fs.adl.oauth2.access.token.provider</name>
          <value>org.apache.hadoop.fs.adls.oauth2.ConfCredentialBasedAccessTokenProvider</value>
      </property>
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://hdfs</value>
      </property>
      <property>
          <name>fs.trash.interval</name>
          <value>1</value>
      </property>
      <property>
          <name>io.compression.codecs</name>
          <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec</value>
      </property>
      <property>
          <name>hadoop.security.authentication</name>
          <value>simple</value>
      </property>
      <property>
          <name>hadoop.security.authorization</name>
          <value>false</value>
      </property>
      <property>
          <name>hadoop.rpc.protection</name>
          <value>authentication</value>
      </property>
      <property>
          <name>hadoop.ssl.require.client.cert</name>
          <value>false</value>
          <final>true</final>
      </property>
      <property>
          <name>hadoop.ssl.keystores.factory.class</name>
          <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
          <final>true</final>
      </property>
      <property>
          <name>hadoop.ssl.server.conf</name>
          <value>ssl-server.xml</value>
          <final>true</final>
      </property>
      <property>
          <name>hadoop.ssl.client.conf</name>
          <value>ssl-client.xml</value>
          <final>true</final>
      </property>
      <property>
          <name>hadoop.security.auth_to_local</name>
          <value>DEFAULT</value>
      </property>
      <property>
          <name>hadoop.proxyuser.hue.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.hue.groups</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.root.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.root.groups</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.HTTP.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.HTTP.groups</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.security.group.mapping</name>
          <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
      </property>
      <property>
          <name>hadoop.security.instrumentation.requires.admin</name>
          <value>false</value>
      </property>
      <property>
          <name>hadoop.proxyuser.httpfs.groups</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.httpfs.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>ha.zookeeper.parent-znode</name>
          <value>/hadoop-0.0.1-hdfs/hadoop-ha</value>
      </property>
      <property>
          <name>fs.s3a.aws.credentials.provider</name>
          <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider,com.amazonaws.auth.InstanceProfileCredentialsProvider</value>
      </property>
      <property>
          <name>hive.exec.orc.split.strategy</name>
          <value>BI</value>
      </property>
      <property>
          <name>fs.s3a.access.key</name>
          <value></value>
      </property>
      <property>
          <name>fs.s3a.secret.key</name>
          <value></value>
      </property>
      <property>
          <name>dfs.adls.oauth2.access.token.provider.type</name>
          <value>ClientCredential</value>
      </property>
      <property>
          <name>dfs.adls.oauth2.client.id</name>
          <value></value>
      </property>
      <property>
          <name>dfs.adls.oauth2.credential</name>
          <value></value>
      </property>
      <property>
          <name>dfs.adls.oauth2.refresh.url</name>
          <value>https://login.microsoftonline.com//oauth2/token</value>
      </property>
      <property>
          <name>fs.gs.impl</name>
          <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
      </property>
      <property>
          <name>fs.gs.working.dir</name>
          <value>/</value>
      </property>
      <property>
          <name>fs.gs.path.encoding</name>
          <value>uri-path</value>
      </property>
      <property>
          <name>fs.gs.auth.service.account.enable</name>
          <value>true</value>
      </property>
      <property>
          <name>fs.gs.auth.service.account.email</name>
          <value></value>
      </property>
      <property>
          <name>fs.gs.auth.service.account.private.key.id</name>
          <value></value>
      </property>
      <property>
          <name>fs.gs.auth.service.account.private.key</name>
          <value></value>
      </property>
    </configuration>
  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>dfs.replication</name>
        <value>3</value>
      </property>
      <property>
        <name>dfs.nameservice.id</name>
        <value>hdfs</value>
      </property>
      <property>
        <name>dfs.nameservices</name>
        <value>hdfs</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.hdfs</name>
        <value>nn0,nn1</value>
      </property>
      <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://foo-hdfs-jn-1.foo-hdfs-jn.default.svc.cluster.local:8485;foo-hdfs-jn-2.foo-hdfs-jn.default.svc.cluster.local:8485;foo-hdfs-jn-0.foo-hdfs-jn.default.svc.cluster.local:8485/hdfs</value>
      </property>
      <property>
        <name>dfs.namenode.safemode.threshold-pct</name>
        <value>0.9</value>
      </property>
      <property>
        <name>dfs.namenode.heartbeat.recheck-interval</name>
        <value>60000</value>
      </property>
      <property>
        <name>dfs.namenode.handler.count</name>
        <value>256</value>
      </property>
      <property>
        <name>dfs.namenode.service.handler.count</name>
        <value>60</value>
      </property>
      <property>
        <name>dfs.namenode.invalidate.work.pct.per.iteration</name>
        <value>0.95</value>
      </property>
      <property>
        <name>dfs.namenode.replication.work.multiplier.per.iteration</name>
        <value>4</value>
      </property>
      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.hdfs.nn0</name>
        <value>foo-hdfs-nn-0.foo-hdfs-nn.default.svc.cluster.local:8020</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.hdfs.nn0</name>
        <value>foo-hdfs-nn-0.foo-hdfs-nn.default.svc.cluster.local:50070</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-bind-host.hdfs.nn0</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.http-bind-host.hdfs.nn0</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.hdfs.nn1</name>
        <value>foo-hdfs-nn-1.foo-hdfs-nn.default.svc.cluster.local:8020</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.hdfs.nn1</name>
        <value>foo-hdfs-nn-1.foo-hdfs-nn.default.svc.cluster.local:50070</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-bind-host.hdfs.nn1</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.http-bind-host.hdfs.nn1</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.journalnode.rpc-address</name>
        <value>0.0.0.0:8485</value>
      </property>
      <property>
        <name>dfs.journalnode.http-address</name>
        <value>0.0.0.0:8480</value>
      </property>
      <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:9003</value>
      </property>
      <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:9004</value>
      </property>
      <property>
        <name>dfs.datanode.ipc.address</name>
        <value>0.0.0.0:9005</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///name-data</value>
      </property>
      <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/journal-data</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///data-data-0</value>
      </property>
      <property>
        <name>dfs.datanode.balance.bandwidthPerSec</name>
        <value>41943040</value>
      </property>
      <property>
        <name>dfs.datanode.handler.count</name>
        <value>20</value>
      </property>
      <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>8192</value>
      </property>
      <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>8192</value>
      </property>
      <property>
        <name>ha.zookeeper.quorum</name>
        <value>foo-zookeeper-1.foo-zookeeper-headless.default.svc.cluster.local:2181,foo-zookeeper-2.foo-zookeeper-headless.default.svc.cluster.local:2181,foo-zookeeper-0.foo-zookeeper-headless.default.svc.cluster.local:2181</value>
      </property>
      <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
      </property>
      <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.image.compress</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.image.compression.codec</name>
        <value>org.apache.hadoop.io.compress.SnappyCodec</value>
      </property>
      <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.client.read.shortcircuit.streams.cache.size</name>
        <value>1000</value>
      </property>
      <property>
        <name>dfs.client.read.shortcircuit.streams.cache.size.expiry.ms</name>
        <value>1000</value>
      </property>
      <property>
        <name>dfs.client.failover.proxy.provider.hdfs</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>
      <property>
        <name>dfs.client.write.retries</name>
        <value>6</value>
      </property>
      <property>
        <name>dfs.client.write.locateFollowingBlock.retries</name>
        <value>10</value>
      </property>
      <property>
        <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
        <value>ALWAYS</value>
      </property>
      <property>
        <name>dfs.namenode.replication.min</name>
        <value>2</value>
      </property>
      <property>
        <name>dfs.domain.socket.path</name>
        <value>/var/lib/hadoop-hdfs/dn_socket</value>
      </property>
      <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.permissions.superusergroup</name>
        <value>hdfs</value>
      </property>
      <property>
        <name>presto.s3.access-key</name>
        <value></value>
      </property>
      <property>
        <name>presto.s3.secret-key</name>
        <value></value>
      </property>
      <property>
        <name>dfs.adls.oauth2.access.token.provider.type</name>
        <value>ClientCredential</value>
      </property>
      <property>
        <name>dfs.adls.oauth2.client.id</name>
        <value></value>
      </property>
      <property>
        <name>dfs.adls.oauth2.credential</name>
        <value></value>
      </property>
      <property>
        <name>dfs.adls.oauth2.refresh.url</name>
        <value>https://login.microsoftonline.com//oauth2/token</value>
      </property>
    </configuration>
